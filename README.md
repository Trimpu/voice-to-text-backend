# Voice-to-Text API Server

A Flask-based REST API that converts audio files to text using OpenAI's Whisper model and provides optional translation services.

## Features

- üéôÔ∏è **Audio Transcription**: Convert audio files to text using OpenAI Whisper
- üåç **Translation**: Translate transcribed text to different languages
- üîÑ **Multiple Audio Formats**: Supports MP3, WAV, and other common audio formats
- üöÄ **REST API**: Simple HTTP endpoints for easy integration
- üê≥ **Cross-platform**: Works on Linux, macOS, and Windows

## Quick Start

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- ffmpeg (for audio processing)

### Installation

1. **Clone or navigate to the project directory**
2. **Run the setup script**:
   ```bash
   ./start_server.sh
   ```
   This will automatically:
   - Create a virtual environment
   - Install all dependencies
   - Start the server

### Manual Setup

If you prefer to set up manually:

1. **Create a virtual environment**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Start the server**:
   ```bash
   python app.py
   ```

## Usage

### API Endpoint

**POST** `/transcribe`

### Parameters

- `file` (required): Audio file to transcribe
- `lang` (optional): Target language for translation (default: "en")

### Example Requests

#### Basic Transcription (English)
```bash
curl -X POST \
  -F 'file=@your_audio.mp3' \
  http://localhost:5000/transcribe
```

#### Transcription with Translation
```bash
curl -X POST \
  -F 'file=@your_audio.mp3' \
  -F 'lang=es' \
  http://localhost:5000/transcribe
```

### Response Format

```json
{
  "original_text": "Hello, this is a test recording.",
  "translated_text": "Hola, esta es una grabaci√≥n de prueba.",
  "language": "es",
  "status": "success"
}
```

### Error Response

```json
{
  "error": "Error description",
  "details": "Detailed error information"
}
```

## Supported Languages

The translation service supports many languages including:
- Spanish (es)
- French (fr)
- German (de)
- Italian (it)
- Portuguese (pt)
- Chinese (zh)
- Japanese (ja)
- Korean (ko)
- And many more...

## Project Structure

```
voice-to-text-backend/
‚îú‚îÄ‚îÄ app.py                 # Main Flask application
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ start_server.sh       # Startup script
‚îú‚îÄ‚îÄ test_app.py           # Test script
‚îú‚îÄ‚îÄ voice_to_text_backend/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ transcription.py  # Whisper integration
‚îÇ   ‚îú‚îÄ‚îÄ translation.py    # Translation service
‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # Utility functions
‚îî‚îÄ‚îÄ README.md             # This file
```

## Dependencies

- **Flask**: Web framework
- **flask-cors**: Cross-origin resource sharing
- **openai-whisper**: Audio transcription
- **requests**: HTTP client for translation API
- **torch**: PyTorch (Whisper dependency)

## Testing

Run the test suite to verify everything is working:

```bash
source venv/bin/activate
python test_app.py
```

## Configuration

### Environment Variables

- `PORT`: Server port (default: 5000)

### Whisper Model

The application uses the "base" Whisper model by default. You can modify this in `voice_to_text_backend/transcription.py`:

```python
model = whisper.load_model("base")  # Options: tiny, base, small, medium, large
```

## Troubleshooting

### Common Issues

1. **ffmpeg not found**:
   ```bash
   # Ubuntu/Debian
   sudo apt install ffmpeg
   
   # macOS
   brew install ffmpeg
   
   # Windows
   # Download from https://ffmpeg.org/download.html
   ```

2. **CUDA/GPU issues**:
   The application will automatically fall back to CPU if CUDA is not available.

3. **Memory issues**:
   If you encounter memory issues, try using a smaller Whisper model:
   ```python
   model = whisper.load_model("tiny")  # Uses less memory
   ```

### Logs

Error logs are printed to the console. For production deployment, consider setting up proper logging.

## Performance

- **First request**: May take longer due to model loading
- **Subsequent requests**: Faster as the model stays in memory
- **File size**: Larger files take longer to process
- **Model size**: Larger models are more accurate but slower

## Production Deployment

For production use, consider:

1. **Use a production WSGI server** (e.g., Gunicorn):
   ```bash
   pip install gunicorn
   gunicorn -w 4 -b 0.0.0.0:5000 app:app
   ```

2. **Set up reverse proxy** (nginx, Apache)
3. **Configure proper logging**
4. **Set up monitoring**
5. **Use environment variables for configuration**

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## Support

If you encounter any issues or have questions, please check the troubleshooting section or create an issue in the repository.